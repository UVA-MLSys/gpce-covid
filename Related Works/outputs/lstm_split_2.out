2023-01-21 22:39:30.049073: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 22:39:30.202165: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 22:39:31.184509: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /u/mi3se/anaconda3/envs/ml/lib:/sw/centos-7.4/anaconda3/current/lib:/sw/centos-7.4/cudnn/current/lib64:/sw/centos-7.4/cuda/current/extras/CUPTI/lib64:/sw/centos-7.4/cuda/current/lib64:/lib::/u/mi3se/anaconda3/envs/ml/lib/
2023-01-21 22:39:31.184618: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /u/mi3se/anaconda3/envs/ml/lib:/sw/centos-7.4/anaconda3/current/lib:/sw/centos-7.4/cudnn/current/lib64:/sw/centos-7.4/cuda/current/extras/CUPTI/lib64:/sw/centos-7.4/cuda/current/lib64:/lib::/u/mi3se/anaconda3/envs/ml/lib/
2023-01-21 22:39:31.184628: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 22:46:13.221839: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 22:46:13.675252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38266 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:c1:00.0, compute capability: 8.0
   FIPS  AgeDist  HealthDisp  ... LinearSpace  SinWeekly  CosWeekly
0  1001   0.1611       4.202  ...         0.0    -0.9749    -0.2225
1  1001   0.1611       4.202  ...         0.0    -0.7818     0.6235
2  1001   0.1611       4.202  ...         0.0     0.0000     1.0000

[3 rows x 14 columns]
Shapes: train (2208826, 14), validation (87976, 14), test (87976, 14).
Shapes: data (2123992, 13, 10), labels (2123992, 15).
Shapes: data (3142, 13, 10), labels (3142, 15).
Shapes: data (3142, 13, 10), labels (3142, 15).
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 13, 64)            19200     
                                                                 
 dropout (Dropout)           (None, 13, 64)            0         
                                                                 
 lstm_1 (LSTM)               (None, 64)                33024     
                                                                 
 dense (Dense)               (None, 15)                975       
                                                                 
=================================================================
Total params: 53,199
Trainable params: 53,199
Non-trainable params: 0
_________________________________________________________________

----Training started at 2023-01-21 22:46:15.255464----

Epoch 1/200
2023-01-21 22:46:18.352269: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8201
2023-01-21 22:46:19.195900: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
16594/16594 - 70s - loss: 0.7059 - val_loss: 1.3984 - 70s/epoch - 4ms/step
Epoch 2/200
16594/16594 - 60s - loss: 0.5604 - val_loss: 1.3343 - 60s/epoch - 4ms/step
Epoch 3/200
16594/16594 - 60s - loss: 0.5127 - val_loss: 1.3620 - 60s/epoch - 4ms/step
Epoch 4/200
16594/16594 - 65s - loss: 0.4801 - val_loss: 1.4073 - 65s/epoch - 4ms/step
Epoch 5/200
16594/16594 - 64s - loss: 0.4579 - val_loss: 1.3501 - 64s/epoch - 4ms/step
Epoch 6/200
16594/16594 - 62s - loss: 0.4408 - val_loss: 1.3474 - 62s/epoch - 4ms/step
Epoch 7/200
16594/16594 - 62s - loss: 0.4265 - val_loss: 1.2791 - 62s/epoch - 4ms/step
Epoch 8/200
16594/16594 - 63s - loss: 0.4143 - val_loss: 1.2603 - 63s/epoch - 4ms/step
Epoch 9/200
16594/16594 - 62s - loss: 0.4015 - val_loss: 1.2533 - 62s/epoch - 4ms/step
Epoch 10/200
16594/16594 - 62s - loss: 0.3913 - val_loss: 1.2688 - 62s/epoch - 4ms/step
Epoch 11/200
16594/16594 - 61s - loss: 0.3819 - val_loss: 1.2707 - 61s/epoch - 4ms/step
Epoch 12/200
16594/16594 - 62s - loss: 0.3736 - val_loss: 1.2261 - 62s/epoch - 4ms/step
Epoch 13/200
16594/16594 - 62s - loss: 0.3688 - val_loss: 1.2657 - 62s/epoch - 4ms/step
Epoch 14/200
16594/16594 - 65s - loss: 0.3592 - val_loss: 1.2256 - 65s/epoch - 4ms/step
Epoch 15/200
16594/16594 - 64s - loss: 0.3521 - val_loss: 1.2608 - 64s/epoch - 4ms/step
Epoch 16/200
16594/16594 - 62s - loss: 0.3467 - val_loss: 1.3092 - 62s/epoch - 4ms/step
Epoch 17/200
16594/16594 - 63s - loss: 0.3404 - val_loss: 1.2118 - 63s/epoch - 4ms/step
Epoch 18/200
16594/16594 - 66s - loss: 0.3349 - val_loss: 1.1613 - 66s/epoch - 4ms/step
Epoch 19/200
16594/16594 - 63s - loss: 0.3281 - val_loss: 1.1789 - 63s/epoch - 4ms/step
Epoch 20/200
16594/16594 - 61s - loss: 0.3237 - val_loss: 1.1470 - 61s/epoch - 4ms/step
Epoch 21/200
16594/16594 - 71s - loss: 0.3207 - val_loss: 1.1028 - 71s/epoch - 4ms/step
Epoch 22/200
16594/16594 - 63s - loss: 0.3139 - val_loss: 1.1578 - 63s/epoch - 4ms/step
Epoch 23/200
16594/16594 - 67s - loss: 0.3118 - val_loss: 1.2024 - 67s/epoch - 4ms/step
Epoch 24/200
16594/16594 - 62s - loss: 0.3071 - val_loss: 1.1256 - 62s/epoch - 4ms/step
Epoch 25/200
16594/16594 - 63s - loss: 0.3030 - val_loss: 1.1393 - 63s/epoch - 4ms/step
Epoch 26/200
16594/16594 - 73s - loss: 0.3006 - val_loss: 1.1426 - 73s/epoch - 4ms/step

----Training ended at 2023-01-21 23:17:12.905860, elapsed time 0:30:57.650396.
Best model by validation loss saved at scratch/results_lstm_split_2/model.h5.
Loading best model.

Train prediction
33188/33188 - 199s - 199s/epoch - 6ms/step
               FIPS         Cases  Predicted_Cases
count  2.167980e+06  2.167980e+06     2.167980e+06
mean   3.038365e+04  2.884061e+01     2.615727e+01
std    1.516010e+04  1.805928e+02     1.194057e+02
min    1.001000e+03  0.000000e+00     0.000000e+00
25%    1.817700e+04  0.000000e+00     1.000000e+00
50%    2.917600e+04  2.000000e+00     3.000000e+00
75%    4.508100e+04  1.300000e+01     1.300000e+01
max    5.604500e+04  2.061825e+04     5.642000e+03
Target Cases, MAE 14.645, RMSE 102.04, RMSLE 1.0808, SMAPE 0.89241. NNSE 0.758.


Validation prediction
50/50 - 0s - 416ms/epoch - 8ms/step
               FIPS         Cases  Predicted_Cases
count  47130.000000  47130.000000     47130.000000
mean   30383.649268     51.054504        77.061935
std    15160.256142    240.272636       235.605940
min     1001.000000      0.000000         0.000000
25%    18177.000000      0.000000         1.000000
50%    29176.000000      6.000000        14.000000
75%    45081.000000     34.000000        56.000000
max    56045.000000  20618.250000      5777.000000
Target Cases, MAE 47.823, RMSE 187.72, RMSLE 1.6362, SMAPE 0.90941. NNSE 0.62096.


Test prediction
50/50 - 0s - 345ms/epoch - 7ms/step
               FIPS         Cases  Predicted_Cases
count  47130.000000  47130.000000     47130.000000
mean   30383.649268     21.205506        75.295672
std    15160.256142    117.881034       172.653501
min     1001.000000      0.000000         0.000000
25%    18177.000000      0.000000         2.000000
50%    29176.000000      1.000000        13.000000
75%    45081.000000     12.000000        55.000000
max    56045.000000  13935.000000      3795.000000
Target Cases, MAE 61.527, RMSE 161.27, RMSLE 2.0471, SMAPE 1.1761. NNSE 0.34823.

Ended at 2023-01-21 23:43:10.521222. Elapsed time 0:56:55.265784

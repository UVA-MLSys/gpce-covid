2023-01-21 22:44:02.510636: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 22:44:02.668903: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2023-01-21 22:44:03.672685: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /u/mi3se/anaconda3/envs/ml/lib:/sw/centos-7.4/anaconda3/current/lib:/sw/centos-7.4/cudnn/current/lib64:/sw/centos-7.4/cuda/current/extras/CUPTI/lib64:/sw/centos-7.4/cuda/current/lib64:/lib::/u/mi3se/anaconda3/envs/ml/lib/
2023-01-21 22:44:03.672793: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /u/mi3se/anaconda3/envs/ml/lib:/sw/centos-7.4/anaconda3/current/lib:/sw/centos-7.4/cudnn/current/lib64:/sw/centos-7.4/cuda/current/extras/CUPTI/lib64:/sw/centos-7.4/cuda/current/lib64:/lib::/u/mi3se/anaconda3/envs/ml/lib/
2023-01-21 22:44:03.672802: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
2023-01-21 22:50:28.393820: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2023-01-21 22:50:28.910579: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38266 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:01:00.0, compute capability: 8.0
   FIPS  AgeDist  HealthDisp  ... LinearSpace  SinWeekly  CosWeekly
0  1001   0.1611       4.202  ...         0.0    -0.9749    -0.2225
1  1001   0.1611       4.202  ...         0.0    -0.7818     0.6235
2  1001   0.1611       4.202  ...         0.0     0.0000     1.0000

[3 rows x 14 columns]
Shapes: train (2111424, 14), validation (87976, 14), test (87976, 14).
Shapes: data (2026590, 13, 10), labels (2026590, 15).
Shapes: data (3142, 13, 10), labels (3142, 15).
Shapes: data (3142, 13, 10), labels (3142, 15).
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 lstm (LSTM)                 (None, 13, 64)            19200     
                                                                 
 dropout (Dropout)           (None, 13, 64)            0         
                                                                 
 lstm_1 (LSTM)               (None, 64)                33024     
                                                                 
 dense (Dense)               (None, 15)                975       
                                                                 
=================================================================
Total params: 53,199
Trainable params: 53,199
Non-trainable params: 0
_________________________________________________________________

----Training started at 2023-01-21 22:50:30.331812----

Epoch 1/200
2023-01-21 22:50:33.503836: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8201
2023-01-21 22:50:34.364233: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
15833/15833 - 67s - loss: 0.6730 - val_loss: 11.8772 - 67s/epoch - 4ms/step
Epoch 2/200
15833/15833 - 56s - loss: 0.5384 - val_loss: 10.7436 - 56s/epoch - 4ms/step
Epoch 3/200
15833/15833 - 57s - loss: 0.4987 - val_loss: 10.1588 - 57s/epoch - 4ms/step
Epoch 4/200
15833/15833 - 58s - loss: 0.4746 - val_loss: 9.6489 - 58s/epoch - 4ms/step
Epoch 5/200
15833/15833 - 72s - loss: 0.4572 - val_loss: 9.2843 - 72s/epoch - 5ms/step
Epoch 6/200
15833/15833 - 57s - loss: 0.4434 - val_loss: 9.1457 - 57s/epoch - 4ms/step
Epoch 7/200
15833/15833 - 57s - loss: 0.4339 - val_loss: 8.9148 - 57s/epoch - 4ms/step
Epoch 8/200
15833/15833 - 66s - loss: 0.4224 - val_loss: 8.8613 - 66s/epoch - 4ms/step
Epoch 9/200
15833/15833 - 62s - loss: 0.4133 - val_loss: 8.7571 - 62s/epoch - 4ms/step
Epoch 10/200
15833/15833 - 63s - loss: 0.4057 - val_loss: 8.6886 - 63s/epoch - 4ms/step
Epoch 11/200
15833/15833 - 57s - loss: 0.4018 - val_loss: 8.5792 - 57s/epoch - 4ms/step
Epoch 12/200
15833/15833 - 59s - loss: 0.3919 - val_loss: 8.5742 - 59s/epoch - 4ms/step
Epoch 13/200
15833/15833 - 58s - loss: 0.3867 - val_loss: 8.3584 - 58s/epoch - 4ms/step
Epoch 14/200
15833/15833 - 56s - loss: 0.3790 - val_loss: 8.2700 - 56s/epoch - 4ms/step
Epoch 15/200
15833/15833 - 58s - loss: 0.3784 - val_loss: 8.2116 - 58s/epoch - 4ms/step
Epoch 16/200
15833/15833 - 58s - loss: 0.3692 - val_loss: 8.0685 - 58s/epoch - 4ms/step
Epoch 17/200
15833/15833 - 58s - loss: 0.3698 - val_loss: 8.0507 - 58s/epoch - 4ms/step
Epoch 18/200
15833/15833 - 59s - loss: 0.3651 - val_loss: 7.9292 - 59s/epoch - 4ms/step
Epoch 19/200
15833/15833 - 57s - loss: 0.3560 - val_loss: 7.7478 - 57s/epoch - 4ms/step
Epoch 20/200
15833/15833 - 57s - loss: 0.3515 - val_loss: 7.5679 - 57s/epoch - 4ms/step
Epoch 21/200
15833/15833 - 57s - loss: 0.3476 - val_loss: 7.4373 - 57s/epoch - 4ms/step
Epoch 22/200
15833/15833 - 69s - loss: 0.3430 - val_loss: 7.5528 - 69s/epoch - 4ms/step
Epoch 23/200
15833/15833 - 57s - loss: 0.3397 - val_loss: 7.5139 - 57s/epoch - 4ms/step
Epoch 24/200
15833/15833 - 58s - loss: 0.3346 - val_loss: 7.5856 - 58s/epoch - 4ms/step
Epoch 25/200
15833/15833 - 59s - loss: 0.3301 - val_loss: 7.2884 - 59s/epoch - 4ms/step
Epoch 26/200
15833/15833 - 57s - loss: 0.3299 - val_loss: 7.3843 - 57s/epoch - 4ms/step
Epoch 27/200
15833/15833 - 64s - loss: 0.3242 - val_loss: 7.4101 - 64s/epoch - 4ms/step
Epoch 28/200
15833/15833 - 57s - loss: 0.3211 - val_loss: 7.2990 - 57s/epoch - 4ms/step
Epoch 29/200
15833/15833 - 58s - loss: 0.3210 - val_loss: 7.3704 - 58s/epoch - 4ms/step
Epoch 30/200
15833/15833 - 56s - loss: 0.3172 - val_loss: 7.0973 - 56s/epoch - 4ms/step
Epoch 31/200
15833/15833 - 57s - loss: 0.3147 - val_loss: 7.1637 - 57s/epoch - 4ms/step
Epoch 32/200
15833/15833 - 57s - loss: 0.3104 - val_loss: 7.2188 - 57s/epoch - 4ms/step
Epoch 33/200
15833/15833 - 61s - loss: 0.3073 - val_loss: 6.9609 - 61s/epoch - 4ms/step
Epoch 34/200
15833/15833 - 57s - loss: 0.3077 - val_loss: 7.1016 - 57s/epoch - 4ms/step
Epoch 35/200
15833/15833 - 57s - loss: 0.3025 - val_loss: 6.7832 - 57s/epoch - 4ms/step
Epoch 36/200
15833/15833 - 57s - loss: 0.2970 - val_loss: 6.9845 - 57s/epoch - 4ms/step
Epoch 37/200
15833/15833 - 68s - loss: 0.2940 - val_loss: 7.0881 - 68s/epoch - 4ms/step
Epoch 38/200
15833/15833 - 57s - loss: 0.2925 - val_loss: 6.7165 - 57s/epoch - 4ms/step
Epoch 39/200
15833/15833 - 57s - loss: 0.2918 - val_loss: 6.6874 - 57s/epoch - 4ms/step
Epoch 40/200
15833/15833 - 62s - loss: 0.2863 - val_loss: 6.8687 - 62s/epoch - 4ms/step
Epoch 41/200
15833/15833 - 57s - loss: 0.2836 - val_loss: 6.8497 - 57s/epoch - 4ms/step
Epoch 42/200
15833/15833 - 70s - loss: 0.2812 - val_loss: 7.0638 - 70s/epoch - 4ms/step
Epoch 43/200
15833/15833 - 56s - loss: 0.2796 - val_loss: 7.0013 - 56s/epoch - 4ms/step
Epoch 44/200
15833/15833 - 57s - loss: 0.2788 - val_loss: 6.8383 - 57s/epoch - 4ms/step

----Training ended at 2023-01-21 23:39:34.840740, elapsed time 0:49:04.508928.
Best model by validation loss saved at scratch/results_LSTM_split_1/model.h5.
Loading best model.

Train prediction
31666/31666 - 190s - 190s/epoch - 6ms/step
               FIPS         Cases  Predicted_Cases
count  2.070578e+06  2.070578e+06     2.070578e+06
mean   3.038365e+04  2.447055e+01     2.260566e+01
std    1.516010e+04  1.430273e+02     1.020859e+02
min    1.001000e+03  0.000000e+00     0.000000e+00
25%    1.817700e+04  0.000000e+00     0.000000e+00
50%    2.917600e+04  2.000000e+00     3.000000e+00
75%    4.508100e+04  1.200000e+01     1.200000e+01
max    5.604500e+04  2.061825e+04     5.928000e+03
Target Cases, MAE 11.769, RMSE 78.64, RMSLE 1.0085, SMAPE 0.76407. NNSE 0.76787.


Validation prediction
50/50 - 0s - 341ms/epoch - 7ms/step
               FIPS         Cases  Predicted_Cases
count  47130.000000  47130.000000     47130.000000
mean   30383.649268    130.366120        89.494101
std    15160.256142    566.621625       360.725859
min     1001.000000      0.000000         0.000000
25%    18177.000000      0.000000         3.000000
50%    29176.000000      8.000000        10.000000
75%    45081.000000     59.500000        35.000000
max    56045.000000  20618.250000      6059.000000
Target Cases, MAE 81.149, RMSE 366.22, RMSLE 1.6914, SMAPE 1.1336. NNSE 0.70535.


Test prediction
50/50 - 0s - 302ms/epoch - 6ms/step
               FIPS         Cases  Predicted_Cases
count  47130.000000  47130.000000     47130.000000
mean   30383.649268    113.286850       109.298791
std    15160.256142    498.940187       329.829993
min     1001.000000      0.000000         0.000000
25%    18177.000000      0.000000         4.000000
50%    29176.000000     10.000000        20.000000
75%    45081.000000     68.000000        72.000000
max    56045.000000  20618.250000      5082.000000
Target Cases, MAE 63.018, RMSE 352.46, RMSLE 1.6926, SMAPE 0.93724. NNSE 0.6671.

Ended at 2023-01-22 00:03:44.366511. Elapsed time 1:13:14.034730
